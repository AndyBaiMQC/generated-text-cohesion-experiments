{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"data_factory.ipynb","provenance":[],"collapsed_sections":["f0ORIhSZkIWr","BumwIoUQkL-A","fJM_Xpxrm2-A"],"authorship_tag":"ABX9TyMSV0ZIvTrTb1dk6POhddL6"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"f0ORIhSZkIWr"},"source":["## Utilities"]},{"cell_type":"code","metadata":{"id":"p02s2M8oj1Vk"},"source":["import pandas as pd\n","import csv\n","import re\n","import os\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WPXn_d4IkReT"},"source":["punc = '''()-[]{}\"\"|\\;:1234567890<>/@#$%^&*+=-_~'''\n","split = 0.8\n","\n","def punc_remove(strng):\n","    for ele in strng:  \n","        if ele in punc:\n","            strng = strng.replace(ele, \"\")\n","    return strng"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BumwIoUQkL-A"},"source":["## Daily Mail"]},{"cell_type":"code","metadata":{"id":"Geg4IMxlkXtm"},"source":["filelist = []\n","directory = r'/content/drive/My Drive/550/data/daily'\n","for filename in os.listdir(directory):\n","    if filename.endswith(\".story\"):\n","        filelist.append(os.path.join(directory, filename))\n","    else:\n","        continue"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b0kl4pOukaUn"},"source":["starter = ['PUBLISHED:', 'By', 'UPDATED:']\n","final = []\n","summary = []\n","content = []\n","\n","for file in filelist:\n","    with open(file) as infile, open('/content/drive/My Drive/550/data/daily/temp.txt', 'w') as outfile:\n","        for line in infile:\n","            if not line.strip(): continue  # skip the empty line\n","            elif line.strip() in starter:\n","                next(infile)\n","            elif re.findall(\"[0-5][0-9]\", line.strip()):\n","                continue\n","            outfile.write(line)  # non-empty line. Write it to output\n","\n","    lst = []\n","    d = {}\n","    s = {}\n","    c = {}\n","    with open('/content/drive/My Drive/550/data/daily/temp.txt','r') as f:\n","        lst = [line.strip() for line in f]\n","        d['agg'] = punc_remove(' '.join(lst[-8:]).replace('@highlight ','') + ' '+' '.join(lst[0:-8])).lower()\n","        s['summary'] = punc_remove(' '.join(lst[-8:]).replace('@highlight ','')).lower()\n","        c['content'] = punc_remove(' '.join(lst[0:-8])).lower()\n","    final.append(d)\n","    summary.append(s)\n","    content.append(c)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"arm_Vq1Xkesw"},"source":["keys = final[0].keys()\n","\n","with open('/content/drive/My Drive/550/data/daily/aggregated.txt', 'w', newline='\\n') as output_file:\n","    dict_writer = csv.DictWriter(output_file, keys)\n","    dict_writer.writerows(final)\n","\n","end = len(open('content/drive/My Drive/550/data/daily/aggregated.txt').readlines( ))\n","count = end * split\n","\n","with open('content/drive/My Drive/550/data/daily/aggregated.txt', 'r') as f:\n","  for i in range(count):\n","    new_sentence = []\n","    sentence = f.readline()\n","    words = sentence.split()\n","    file_object = open('/content/drive/My Drive/550/data/daily/aggregated_train.txt', 'a')\n","    file_object.write(' '.join(new_sentence)+'\\n')\n","  file_object.close()\n","f.close()\n","\n","with open('content/drive/My Drive/550/data/daily/aggregated.txt', 'r') as f:\n","  for i in range(count, end):\n","    new_sentence = []\n","    sentence = f.readline()\n","    words = sentence.split()\n","    file_object = open('/content/drive/My Drive/550/data/daily/aggregated_test.txt', 'a')\n","    file_object.write(' '.join(new_sentence)+'\\n')\n","  file_object.close()\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WP4WKRtPmUXw"},"source":["keys = summary[0].keys()\n","\n","with open('/content/drive/My Drive/550/data/daily/summary.txt', 'w', newline='\\n') as output_file:\n","    dict_writer = csv.DictWriter(output_file, keys)\n","    dict_writer.writerows(summary)\n","\n","end = len(open('summary/drive/My Drive/550/data/daily/summary.txt').readlines( ))\n","count = end * split\n","\n","with open('summary/drive/My Drive/550/data/daily/summary.txt', 'r') as f:\n","  for i in range(count):\n","    new_sentence = []\n","    sentence = f.readline()\n","    words = sentence.split()\n","    file_object = open('/summary/drive/My Drive/550/data/daily/summary_train.txt', 'a')\n","    file_object.write(' '.join(new_sentence)+'\\n')\n","  file_object.close()\n","f.close()\n","\n","with open('summary/drive/My Drive/550/data/daily/summary.txt', 'r') as f:\n","  for i in range(count, end):\n","    new_sentence = []\n","    sentence = f.readline()\n","    words = sentence.split()\n","    file_object = open('/summary/drive/My Drive/550/data/daily/summary_test.txt', 'a')\n","    file_object.write(' '.join(new_sentence)+'\\n')\n","  file_object.close()\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rO-K3UaAmcQI"},"source":["keys = content[0].keys()\n","\n","with open('/content/drive/My Drive/550/data/daily/content.txt', 'w', newline='\\n') as output_file:\n","    dict_writer = csv.DictWriter(output_file, keys)\n","    dict_writer.writerows(content)\n","\n","end = len(open('content/drive/My Drive/550/data/daily/content.txt').readlines( ))\n","count = end * split\n","\n","with open('content/drive/My Drive/550/data/daily/content.txt', 'r') as f:\n","  for i in range(count):\n","    new_sentence = []\n","    sentence = f.readline()\n","    words = sentence.split()\n","    file_object = open('/content/drive/My Drive/550/data/daily/content_train.txt', 'a')\n","    file_object.write(' '.join(new_sentence)+'\\n')\n","  file_object.close()\n","f.close()\n","\n","with open('content/drive/My Drive/550/data/daily/content.txt', 'r') as f:\n","  for i in range(count, end):\n","    new_sentence = []\n","    sentence = f.readline()\n","    words = sentence.split()\n","    file_object = open('/content/drive/My Drive/550/data/daily/content_test.txt', 'a')\n","    file_object.write(' '.join(new_sentence)+'\\n')\n","  file_object.close()\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hl_8roDilVY6"},"source":["### Delete"]},{"cell_type":"code","metadata":{"id":"y1LsnW9fk6Mh"},"source":["!rm '/content/drive/My Drive/550/data/daily/temp.txt'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fJM_Xpxrm2-A"},"source":["## Reddit"]},{"cell_type":"code","metadata":{"id":"i70KlAVDm5YV"},"source":["lst = []\n","summary = []\n","content = []\n","\n","num_lines = 3050000\n","\n","with open('/content/drive/My Drive/550/data/reddit/full.jsonl', 'r') as f:\n","  for i in range(num_lines):\n","    newdict = {}\n","    summaries = {}\n","    contents = {}\n","    test_str = f.readline()\n","    mydict = json.loads(test_str)\n","    newdict['full'] = punc_remove(mydict['content']+' '+mydict['normalizedBody']).lower()\n","    summaries['content'] = punc_remove(mydict['content']).lower()\n","    contents['normalizedBody'] = punc_remove(mydict['normalizedBody']).lower()\n","    lst.append(newdict)\n","    summaries.append(summaries)\n","    contents.append(contents)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uW6ffLekndf8"},"source":["keys = lst[0].keys()\n","\n","with open('/content/drive/My Drive/550/data/reddit/aggregated.txt', 'w', newline='\\n') as output_file:\n","    dict_writer = csv.DictWriter(output_file, keys)\n","    dict_writer.writerows(lst)\n","\n","end = len(open('content/drive/My Drive/550/data/reddit/aggregated.txt').readlines( ))\n","count = end * split\n","\n","with open('content/drive/My Drive/550/data/reddit/aggregated.txt', 'r') as f:\n","  for i in range(count):\n","    new_sentence = []\n","    sentence = f.readline()\n","    words = sentence.split()\n","    file_object = open('/content/drive/My Drive/550/data/reddit/aggregated_train.txt', 'a')\n","    file_object.write(' '.join(new_sentence)+'\\n')\n","  file_object.close()\n","f.close()\n","\n","with open('content/drive/My Drive/550/data/reddit/aggregated.txt', 'r') as f:\n","  for i in range(count, end):\n","    new_sentence = []\n","    sentence = f.readline()\n","    words = sentence.split()\n","    file_object = open('/content/drive/My Drive/550/data/reddit/aggregated_test.txt', 'a')\n","    file_object.write(' '.join(new_sentence)+'\\n')\n","  file_object.close()\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6BrXe9gIoVpD"},"source":["keys = summaries[0].keys()\n","\n","with open('/content/drive/My Drive/550/data/reddit/summary.txt', 'w', newline='\\n') as output_file:\n","    dict_writer = csv.DictWriter(output_file, keys)\n","    dict_writer.writerows(summaries)\n","\n","end = len(open('content/drive/My Drive/550/data/reddit/summary.txt').readlines( ))\n","count = end * split\n","\n","with open('content/drive/My Drive/550/data/reddit/summary.txt', 'r') as f:\n","  for i in range(count):\n","    new_sentence = []\n","    sentence = f.readline()\n","    words = sentence.split()\n","    file_object = open('/content/drive/My Drive/550/data/reddit/summary_train.txt', 'a')\n","    file_object.write(' '.join(new_sentence)+'\\n')\n","  file_object.close()\n","f.close()\n","\n","with open('content/drive/My Drive/550/data/reddit/aggregated.txt', 'r') as f:\n","  for i in range(count, end):\n","    new_sentence = []\n","    sentence = f.readline()\n","    words = sentence.split()\n","    file_object = open('/content/drive/My Drive/550/data/reddit/summary_test.txt', 'a')\n","    file_object.write(' '.join(new_sentence)+'\\n')\n","  file_object.close()\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"36hTwRG-obcX"},"source":["keys = contents[0].keys()\n","\n","with open('/content/drive/My Drive/550/data/reddit/content.txt', 'w', newline='\\n') as output_file:\n","    dict_writer = csv.DictWriter(output_file, keys)\n","    dict_writer.writerows(contents)\n","\n","end = len(open('content/drive/My Drive/550/data/reddit/content.txt').readlines( ))\n","count = end * split\n","\n","with open('content/drive/My Drive/550/data/reddit/content.txt', 'r') as f:\n","  for i in range(count):\n","    new_sentence = []\n","    sentence = f.readline()\n","    words = sentence.split()\n","    file_object = open('/content/drive/My Drive/550/data/reddit/content_train.txt', 'a')\n","    file_object.write(' '.join(new_sentence)+'\\n')\n","  file_object.close()\n","f.close()\n","\n","with open('content/drive/My Drive/550/data/reddit/content.txt', 'r') as f:\n","  for i in range(count, end):\n","    new_sentence = []\n","    sentence = f.readline()\n","    words = sentence.split()\n","    file_object = open('/content/drive/My Drive/550/data/reddit/content_test.txt', 'a')\n","    file_object.write(' '.join(new_sentence)+'\\n')\n","  file_object.close()\n","f.close()"],"execution_count":null,"outputs":[]}]}