{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"generate_fine_tuned_models.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"RpgAg3YVJ36V"},"source":["## Data Clean Up"]},{"cell_type":"code","metadata":{"id":"HAHOJAYSDI7e"},"source":["!pip install transformers\n","!pip install spacy\n","!pip install PyDictionary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NdWfkKOxDPPb"},"source":["import logging\n","import os\n","import pickle\n","\n","import torch\n","import torch.nn as nn\n","import transformers\n","from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n","from transformers import (\n","    GPT2Config,\n","    GPT2LMHeadModel,\n","    GPT2PreTrainedModel,\n","    GPT2Tokenizer,\n","    PreTrainedModel,\n","    PreTrainedTokenizer,\n",")\n","\n","file_path = \"/content/drive/My Drive/550/data/reddit/aggregated_train.csv\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tBomIH98J11x"},"source":["## Dataloader"]},{"cell_type":"code","metadata":{"id":"RKwr9LGjEhqI"},"source":["logger = logging.getLogger(__name__)\n","\n","class ScriptData(Dataset):\n","    def __init__(\n","        self,\n","        tokenizer: PreTrainedTokenizer,\n","        file_path: str,\n","        block_size=512,\n","        overwrite_cache=False,\n","    ):\n","        assert os.path.isfile(file_path)\n","\n","        block_size = block_size - (\n","            tokenizer.max_len - tokenizer.max_len_single_sentence\n","        )\n","\n","        directory, filename = os.path.split(file_path)\n","\n","        # change if args are added at later point\n","        cached_features_file = os.path.join(\n","            directory, \"gpt2\" + \"_\" + str(block_size) + \"_\" + filename\n","        )\n","\n","        if os.path.exists(cached_features_file) and not overwrite_cache:\n","            logger.info(\n","                f\"Loading features from your cached file {cached_features_file}\"\n","            )\n","            with open(cached_features_file, \"rb\") as cache:\n","                self.examples = pickle.load(cache)\n","                logger.debug(\"Loaded examples from cache\")\n","        else:\n","            logger.info(f\"Creating features from file {filename} at {directory}\")\n","\n","            self.examples = []\n","            with open(file_path, encoding=\"utf-8\") as f:\n","                text = f.read()\n","                logger.debug(\"Succesfully read text from file\")\n","\n","            tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n","\n","            for i in range(0, len(tokenized_text) - block_size + 1, block_size):\n","                self.examples.append(\n","                    tokenizer.build_inputs_with_special_tokens(\n","                        tokenized_text[i : i + block_size]\n","                    )\n","                )\n","\n","            logger.info(f\"Saving features into cached file {cached_features_file}\")\n","            with open(cached_features_file, \"wb\") as cache:\n","                \n","                pickle.dump(self.examples, cache, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, item):\n","        return torch.tensor(self.examples[item], dtype=torch.long)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YPxbC8_1JwKC"},"source":["## LOAD MODEL"]},{"cell_type":"code","metadata":{"id":"1T3lO_8LEr2k"},"source":["device = 'cpu'\n","if torch.cuda.is_available():\n","    device = 'cuda'\n","\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lKn-xuYsJyyc"},"source":["### Using a custom model to finetune"]},{"cell_type":"code","metadata":{"id":"qRwWev77ExLA"},"source":["from torch.utils.data import Dataset, DataLoader\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","dataset = ScriptData(tokenizer=tokenizer, file_path=file_path)\n","script_loader = DataLoader(dataset,batch_size=4,shuffle=True)\n","\n","BATCH_SIZE = 1\n","EPOCHS = 10\n","LEARNING_RATE = 0.00002\n","WARMUP_STEPS = 10000\n","\n","model = model.to(device)\n","model.train()\n","optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=-1)\n","script_count = 0\n","sum_loss = 0.0\n","batch_count = 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UYvInSKcJk4h"},"source":["## TRAIN GPT (FINETUNE)"]},{"cell_type":"code","metadata":{"id":"MS7F2r_xd-tg"},"source":["import random"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mkQDp-msF4lT"},"source":["\n","for epoch in range(10):\n","    print(f\"EPOCH {epoch} started\" + '=' * 30)\n","    for idx,script in enumerate(script_loader):\n","        outputs = model(script.to(device), labels=script.to(device))\n","        \n","        loss, logits = outputs[:2]                        \n","        loss.backward()\n","        sum_loss = sum_loss + loss.detach().data\n","                       \n","        script_count = script_count + 1\n","        if script_count == BATCH_SIZE:\n","            script_count = 0    \n","            batch_count += 1\n","            optimizer.step()\n","            scheduler.step() \n","            optimizer.zero_grad()\n","            model.zero_grad()\n","\n","        # print(f\"Batch Count: {batch_count}\")\n","\n","        if batch_count == 500:\n","            # print(\"in batch count\")\n","            model.eval()\n","            print(f\"sum loss {sum_loss}\")\n","            sample_outputs = model.generate(\n","                                    bos_token_id=random.randint(1,30000),\n","                                    do_sample=True,   \n","                                    top_k=50, \n","                                    max_length = 50,\n","                                    top_p=0.95, \n","                                    num_return_sequences=1\n","                                )\n","\n","            print(\"Output:\\n\" + 100 * '-')\n","            for i, sample_output in enumerate(sample_outputs):\n","                  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n","            \n","            batch_count = 0\n","            sum_loss = 0.0\n","            model.train()\n","\n","    if (epoch+1)%2==0:\n","      torch.save(model, f\"/content/drive/My Drive/550/output/gpt/reddit_model_{epoch+1+4}\")"],"execution_count":null,"outputs":[]}]}